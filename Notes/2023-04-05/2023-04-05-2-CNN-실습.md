---
title: 2. CNN 실습
---

# CNN 실습

## 1.  MNIST를 Multinomial Classification으로 구현

```python
# Import Modules
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Raw Data Loading
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/KOSA 실습/data/mnist_train.csv')
# display(df.head(), df.shape)
# 이 데이터는 샘플이라서 결측치와 이상치가 없음. 정규화는 해야함

# 데이터 나누기
x_data_train, x_data_test, t_data_train, t_data_test = \
train_test_split(df.drop('label', axis=1, inplace=False),
                 df['label'],
                 test_size=0.3) # train 7 : test 3

# 정규화 진행
scaler = MinMaxScaler()
scaler.fit(x_data_train)
x_data_train_norm = scaler.transform(x_data_train)
x_data_test_norm = scaler.transform(x_data_test)
```

```python
# Multinomial Classification
model_1 = Sequential()
model_1.add(Flatten(input_shape=(784,)))
model_1.add(Dense(10, activation='softmax'))

model_1.compile(optimizer=Adam(learning_rate=1e-4),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss',
                               patience=3,
                               verbose=1,
                               mode='auto',
                               restore_best_weights=True)

model_1.fit(x_data_train_norm,
            t_data_train,
            epochs=2000,
            validation_split=0.2,
            verbose=1,
            batch_size=100,
            callbacks=[early_stopping])
```

```python
# model_1 평가
print(model_1.evaluate(x_data_test_norm, t_data_test)) # [0.26968684792518616, 0.9232539534568787]
```

정확도는 0.92 정도이다.

## 2.  MNIST를 DNN으로 구현

```python
# === 2. DNN ===

model_2 = Sequential()

model_2.add(Flatten(input_shape=(784,)))

# Hidden Layer
model_2.add(Dense(64, activation='relu')) # 노드 수는 우선 임의로 지정. 관련 연구들이 나와 있음.
model_2.add(Dense(128, activation='relu'))
model_2.add(Dense(32, activation='relu'))

model_2.add(Dense(10, activation='softmax'))

model_2.compile(optimizer=Adam(learning_rate=1e-4),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss',
                               patience=3,
                               verbose=1,
                               mode='auto',
                               restore_best_weights=True)

model_2.fit(x_data_train_norm,
            t_data_train,
            epochs=2000,
            validation_split=0.2,
            verbose=1,
            batch_size=100,
            callbacks=[early_stopping])
```

```python
# model_2 평가
print(model_2.evaluate(x_data_test_norm, t_data_test)) # [0.13501515984535217, 0.9588888883590698]
```

0.92에서 0.95로 성능이 올라갔다.

## 3.  MNIST를 CNN으로 구현

```python
# === 3. CNN ===

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout

model_3 = Sequential()

# Feature Extraction (Convolution 처리)
# 첫 Convolution Layer는 Input Layer를 겸함 
model_3.add(Conv2D(filters=32, # 필터 개수
                   kernel_size=(3, 3), # 필터 크기
                   activation='relu', # 연산 후 relu로 전파
                   input_shape=(28, 28, 1))) # 4차원 형태로 입력 = 3차원 이미지 데이터(가로, 세로, 색상) 여러개 

model_3.add(MaxPooling2D(pool_size=(2, 2))) # Pooling 할 크기

## 반복
model_3.add(Conv2D(filters=64, # 테스트이므로 임의로 지정함
                   kernel_size=(3, 3),
                   activation='relu')) # 두번째 Conv Layer부터는 input_shape 없음

model_3.add(MaxPooling2D(pool_size=(2, 2))) # Pooling 할 크기

## 반복
model_3.add(Conv2D(filters=64,
                   kernel_size=(3, 3),
                   activation='relu'))

model_3.add(MaxPooling2D(pool_size=(2, 2))) # Pooling 할 크기

# (DNN)
model_3.add(Flatten()) # input_shape을 잡지 않음. 자동 변환
model_3.add(Dense(128, activation='relu'))
model_3.add(Dense(10, activation='softmax')) # 분류기

model_3.compile(optimizer=Adam(learning_rate=1e-4),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss',
                               patience=3,
                               verbose=1,
                               mode='auto',
                               restore_best_weights=True)

model_3.fit(x_data_train_norm.reshape(-1, 28, 28, 1), # 4차원 형태로 reshape. => 28*28의 픽셀데이터 여러(-1)개
            t_data_train,
            epochs=2000,
            validation_split=0.2,
            verbose=1,
            batch_size=100,
            callbacks=[early_stopping])
```

```python
# model_3 평가
print(model_3.evaluate(x_data_test_norm.reshape(-1, 28, 28, 1),
                       t_data_test)) # [0.08553112298250198, 0.9743650555610657]
```

정확도가 0.97 정도로 상승하였다.

## 결론

CNN을 이용하여 Model을 학습하면 Image 학습, 예측에 좋은 결과를 얻을 수 있다.

- 데이터는 4차원 형태로 입력해야 한다. (3차원 형태의 이미지 데이터 여러 개)
- Model을 만드는데 사용한 라이브러비: Keras
- Model 학습, 평가에 사용한 라이브러리: Keras

실제로 실사 이미지 분류 모델을 만들기 위해서는 생각보다 많은 과정이 필요하다.
- 우선 데이터셋을 구해야한다. 대량의 이미지를 수집해야 한다.
- 수집한 이미지에 대해 라벨링 작업을 해야한다.
- 수집한 이미지 각각을 같은 사이즈로 리사이징 해야한다.

이러한 수고로움을 조금 줄여주기 위해 전이학습을 활용할 수 있다. 관련해서는 내일 살펴본다.

