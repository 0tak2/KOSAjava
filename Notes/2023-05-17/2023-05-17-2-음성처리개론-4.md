---
title: 음성처리개론 4
---

# 음성처리개론 4

## 음성인식

–  음성인식의 발전
    - 전통적인 확률적 방식 GMM + HMM
    - 음향 모델, 언어 모델 등 딥러닝 모듈을 사용하는 파이프라인
    - 종단간 모델 (end-to-end)
           - 위스퍼: OpenAI. 구조적으로 트랜스포머 구조.

- 전통적인 확률적 방식 GMM + HMM
    - 마코프 체인
        - MLE 방법론의 일종
        - 한 상태에서 다른 상태로 전이할 때 직전 상태에 기반하여 다음 상태를 예측
        - 오늘 쨍쨍할 때, 내일 흐릴 확률은?
        - ㅇ이 발음되었을 때, ㅏ가 발음될 확률은?
    - HMM (히든 마코프 체인)
        - 각 상태에서 다른 상태로 전이될 확률은 숨겨져있다Hidden고 가정하고 주어진 데이터에 대해 이 전이 확률을 최적화
    - GMM
        - HMM의 전이 유형 중 방출 확률을 구하는 데 사용
    - 한계: 직전 상태만 고려함.

- 딥러닝의 도임
    - 구성요소: 음향모델, 언어모델, 발음사전
    - 음성 입력 -> 특징 추출 -> 음향모델 -> 디코딩 -> 텍스트 출력  
                                                                        ^ 언어모델
                                                                        ^ 발음사전
    - [꼳] -> 꼿? 꽃? 꽂?
    - CTC: 음성 데이터와 레이블 사이 정렬 정보가 없어도 학습이 가능하도록 만드는 알고리즘
        - 음소, 음운 발음과 대응되는 레이블 찾기
        - 탐색 알고리즘이 사용하여 가능한 모든 시퀀스로부터 최적의 정렬 상태를 찾아냄
        - [참고](https://distill.pub/2017/ctc/)
        - 대각선 상/하향 그래프

- 종단간모델
    - 위스퍼 (2022, OpenAI)
    - 언어모델을 통한 결과 보정 없어짐

- 음성 인식의 평가지표
    - CER (Character Error Rate): 에측과 정답 사이 문자 단위 오류 비율
    - WER (Word): 단어 단위

## 음성합성

–  음성합성의 발전
    - 전통적인 음편 선정 방식
    - 음향 모델 + 보코더
        - 옛날에는 이를 종단간 모델이라고 했으나, 요즘은 이를 파이프라인, 캐서캐이드 모델이라고 부름
    - 종단간모델
        - Flow: 카카오 엔터프라이즈

- 전통적 음편 선정방식
    - 최소한의 구분가능한 음소 분할: 5ms
        - 하나의 음소이더라도 어디에 붙는지 등에 따라 달라지므로 디테일하게 데이터를 구축하기도 했음.
        - 이 경우 어떤 상황에 어떤 음편을 붙이는게 적당한지를 판단하는 것은 또한 전문적인 분야
    - 아무리 데이터를 디테일하게 만들고, 음편 선정을 자세하게 해도 붙여서 만들면 어색할 수밖에 없음

- 캐스케이드 모델
    - 음향모델 + 파이프라인

- 연구 사례
    - Tacotron (2017, Google)
        - 보코더까지는 제안을 안했고 무슨 그리핀 림 방식(통계적) 통해서 멜스펙토그램을 음성으로 복원
        - 자기회귀 방식: 생성 직전 시점 어텐션 값을 현재 시점에 이용 -> 이전에 틀리면 계속 틀림
        - 인코더: 문자로부터 특징을 추출하여 벡터화
        - 어텐션
            - 벡터로부터 시간별 발화 문자(순서) 정보 추출
            - 한글은 초중종성 분리돼 입력됨
        - 디코더
            - 선형 스펙트로그램 생성
    - Tacotron2 (2017, Google)
        - 구조적으로 개선 - 디코더에 LSTM 사용
        - 보코더(WaveNet) 도임
        - 1보다 선명해짐
    - 자기회귀 vs 비자기회귀
        - 자기회귀 방식은 이전 시점 데이터를 거듭 참조하므로 합성 속도가 느리고 이전에 틀리면 계속 틀리게 되는 한계점
        - 이를 비자기회귀 방식으로 극복
        - 비자기회귀 방식: 순방향 트랜스포머 사용. 속도 빨라졌으나 다소 어색한 억양
            - 문장이 길어지면 머머링 현상 발생
    - FastSpeech 2 (2020, Microsoft)
        - 억양이 다소 어색 (끝나는 지점의 억양, 두문자 읽기)
        - 대신 합성 속도는 빨라짐
        - 문자-음성 간 정렬 정보 외에 추가 정보 학습
            - 음소당 발화 길이, 세기--에너지/명확도, 음조(피치)
    - FastPitch (2020, NVIDIA)
        - FastSpeech 2 기반. 음색 모사가 잘됨
        - 오히려 경량화되어 품질이 좋아짐

- 최근 - 초거대 생성모델
    - State Of The Art; SOTA
        - AudioLM (2022, Google)
            - 프롬프트를 주면 발화 생성
        - MusicLM (2023, Google)
            - 음악을 생성
        - VALL-E
            - 텍스트 프롬프트와 오디오 프롬프트(3초만 사용)를 받음
            - 오디오 프롬프트는 오디오 코덱 인코더를 통해 8자리 코드로 인코딩됨
            - 각 인코더와 디코더는 다른 모델에 기반함
            - 원래 전사가 없는 대용량 데이터셋을 기반으로 CTC 등으로 저품질이더라도 전사를 확보하여 학습
                - 전통적인 방식(사운드 엔지니어 통제 하에 고품질 오디오와 전사를 활용)과 다른 새로운 방식
            - VALL-E X: 다국어 학습. 한국어 화자가 영어를 말한다든지가 가능





